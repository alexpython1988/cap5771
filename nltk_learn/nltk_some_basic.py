# part of code for learning how to use basic functions in NLTK toolkit

#import nltk
# nltk.download()
from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer
from nltk.corpus import stopwords, state_union, gutenberg, wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag, RegexpParser, ne_chunk

def tokenize():
    # tokenizing - word tokenizer: seperate by word -sentence tokenizer: seperate by sentence -lexicon: words and meaning -corpora: body of text
    example1 = "Hello there Mr.Smith, how are you doing today? The weather is great. The sky is pinkish blue and you should not eat donut."
    # print(sent_tokenize(example1))
    # print(word_tokenize(example1))
    for each in sent_tokenize(example1):
        print(each)

    for each in word_tokenize(example1):
        print(each)


def stop_words_filter():
    # stop words -> words you do not need like prop.
    ex1 = "This is an example showing off stop word filtration."
    stop_words = set(stopwords.words("english"))
    # print(stop_words)
    words = word_tokenize(ex1)

    # implement in for loop
    # for each in words:
    # 	if each not in stop_words:
    # 		filtered_words.append(each)

    # shorter implementation
    # filtered_words = [w for w in words if not w in stop_words]

    # use filter
    filtered_words = list(filter(lambda w: w not in stop_words, words))

    print(filtered_words)


def steming():
    # get the stem of the words
    ps = PorterStemmer()

    example_words = ["python", "pythoner", "pythoning",
                     "pythoned", "pythonly", "pythonization"]
    stemed_words_compare = list(
        zip(example_words, list(map(ps.stem, example_words))))
    print(stemed_words_compare)

    example_sentence = "It is very important to be carefully while you are pythoning with python. All pythoner have pythoned poorly at least once."
    words_from_es = word_tokenize(example_sentence)
    stop_words = set(stopwords.words("english"))
    filtered_stemed_words_es = list(
        map(ps.stem, list(filter(lambda w: w not in stop_words, words_from_es))))
    print(filtered_stemed_words_es)


def speech_tagging():
    # train a speech model to tokenized the speech sentence and  tokenize and tag each words in each sentence
    train_text = state_union.raw("2005-GWBush.txt")
    test_text = state_union.raw("2006-GWBush.txt")
    # print(train_text)
    customed_sent_tokenizer = PunktSentenceTokenizer(train_text)
    tokenized = customed_sent_tokenizer.tokenize(test_text) #list of sentences
    # print(tokenized)
    tagged_data = __process_content(tokenized)
    return tagged_data

def __process_content(tokenized_sent):
    # label every words not in stop words and provide a tag for each of them with word properties
    # such as apple will be labeled as NNP -> norn, big as JJ -> adjective and run as VB -> verb
    tagged_list = []
    stop_words = set(stopwords.words("english"))
    for each_sent in tokenized_sent[5:]:
        filtered_words = list(
            filter(lambda w: w not in stop_words, word_tokenize(each_sent)))
        tagged = pos_tag(filtered_words)
        #print(tagged)
        tagged_list.append(tagged)

    return tagged_list

def chunking(tagged_data):
    #process the meaning of the sentence
    #usurally chunk to noun by regex -> grouping
    #<XX> -> XX denotes labeled tags generated by tag methods (tagging stage)
    chunkGram = r'''
        Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}
    ''' 

    chunk_parser = RegexpParser(chunkGram)
    chunked = chunk_parser.parse(tagged_data)
    print(chunked)
    #chunked.draw()

def chinking(tagged_data):
    # remove chunks from chunk you do not need (chink is also regular expression)
    # chink works with chunk together
    #the first line is chunk {} and the second line is chink }{
    chunkGram = r'''
        Chunk: {<.*>+}
                }<VB.*?|IN|DT|>+{
    ''' 
    chunk_parser = RegexpParser(chunkGram)
    chunked = chunk_parser.parse(tagged_data)
    print(chunked)

def name_endentity_chunk(tagged_data):
    # name identity recognization
    ne_en = ne_chunk(tagged_data, binary=True) #when binary is True -> no specific category like PERSON, ORGANIZATION, LOCATION... will show
    print(ne_en)
    #ne_en.draw()

def lemmatizer():
    # use more often than steming
    # without pos parameter, lemmatizer will check and change nouns's plural form to single form
    sample = ["Eric", "cats", "buildings", "ran", "running", "geese", "rocks", "feet", "better", "best"]
    lemmatizer = WordNetLemmatizer()
    ps = PorterStemmer()
    for each in sample:
        print(lemmatizer.lemmatize(each))
        print(ps.stem(lemmatizer.lemmatize(each)))

    # with pos as 'a' -> adj -> it will change the compare form to original form as example below: better to good
    # with pos as "v" -> verb
    # default pos is 'n' -> noun
    print(lemmatizer.lemmatize("better", pos='a'))
    print(lemmatizer.lemmatize("larger", pos='a'))
    print(lemmatizer.lemmatize("ran", pos='v'))
    print(lemmatizer.lemmatize("thought", pos='v'))

def corpus_test():
    #nltk.corpus has many sample text you can import -> from nltk.corpus import module name
    sample_text = gutenberg.raw("bible-kjv.txt")
    sent_tokens = sent_tokenize(sample_text)
    print(sent_tokens[5:15])

def wordnet_test():
    #synset -> synonyms of the word as the arg
    syns = wordnet.synsets("program")
    print(syns)
    print(syns[1].lemmas())
    print(syns[1].lemmas()[0].name())
    print(syns[1].definition())
    print(syns[0].examples())

    synss = []
    anyss = []

    for syn in wordnet.synsets("good"):
        for l in syn.lemmas():
            synss.append(l.name())
            if l.antonyms():
                anyss.append(l.antonyms()[0].name())

    print(set(synss))
    print(set(anyss))

    w1 = wordnet.synset("ship.n.01")
    w2 = wordnet.synset("boat.n.01")
    #measure meaning similarity
    print("the meaning similarity between {} and {} is {:.2f}".format(w1, w2, w1.wup_similarity(w2)))

    w3 = wordnet.synset("car.n.01")
    #measure meaning similarity
    print("the meaning similarity between {} and {} is {:.2f}".format(w1, w3, w1.wup_similarity(w3)))

    w4 = wordnet.synset("human.n.01")
    #measure meaning similarity
    print("the meaning similarity between {} and {} is {:.2f}".format(w1, w4, w1.wup_similarity(w4)))

def main():
    # tokenize()
    
    # stop_words_filter()
    
    # steming()
    
    # tagged_data = speech_tagging()
    # for i, each in enumerate(tagged_data):
    #     if i == 1:
    #         chunking(each)
    #         chinking(each)
    #         name_endentity_chunk(each)

    #lemmatizer()
    #corpus_test()
    wordnet_test()

if __name__ == '__main__':
    main()
