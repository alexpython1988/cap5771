# part of code for learning how to use basic functions in NLTK toolkit

#import nltk
# nltk.download()
from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer
from nltk.corpus import stopwords, state_union, gutenberg, wordnet, movie_reviews
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag, RegexpParser, ne_chunk, FreqDist, NaiveBayesClassifier
from nltk.classify import accuracy, ClassifierI
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from statistics import mode
import random
import pickle

def tokenize():
    # tokenizing - word tokenizer: seperate by word -sentence tokenizer: seperate by sentence -lexicon: words and meaning -corpora: body of text
    example1 = "Hello there Mr.Smith, how are you doing today? The weather is great. The sky is pinkish blue and you should not eat donut."
    # print(sent_tokenize(example1))
    # print(word_tokenize(example1))
    for each in sent_tokenize(example1):
        print(each)

    for each in word_tokenize(example1):
        print(each)


def stop_words_filter():
    # stop words -> words you do not need like prop.
    ex1 = "This is an example showing off stop word filtration."
    stop_words = set(stopwords.words("english"))
    # print(stop_words)
    words = word_tokenize(ex1)

    # implement in for loop
    # for each in words:
    # 	if each not in stop_words:
    # 		filtered_words.append(each)

    # shorter implementation
    # filtered_words = [w for w in words if not w in stop_words]

    # use filter
    filtered_words = list(filter(lambda w: w not in stop_words, words))

    print(filtered_words)


def steming():
    # get the stem of the words
    ps = PorterStemmer()

    example_words = ["python", "pythoner", "pythoning",
                     "pythoned", "pythonly", "pythonization"]
    stemed_words_compare = list(
        zip(example_words, list(map(ps.stem, example_words))))
    print(stemed_words_compare)

    example_sentence = "It is very important to be carefully while you are pythoning with python. All pythoner have pythoned poorly at least once."
    words_from_es = word_tokenize(example_sentence)
    stop_words = set(stopwords.words("english"))
    filtered_stemed_words_es = list(
        map(ps.stem, list(filter(lambda w: w not in stop_words, words_from_es))))
    print(filtered_stemed_words_es)


def speech_tagging():
    # train a speech model to tokenized the speech sentence and  tokenize and tag each words in each sentence
    train_text = state_union.raw("2005-GWBush.txt")
    test_text = state_union.raw("2006-GWBush.txt")
    # print(train_text)
    customed_sent_tokenizer = PunktSentenceTokenizer(train_text)
    tokenized = customed_sent_tokenizer.tokenize(test_text) #list of sentences
    # print(tokenized)
    tagged_data = __process_content(tokenized)
    return tagged_data

def __process_content(tokenized_sent):
    # label every words not in stop words and provide a tag for each of them with word properties
    # such as apple will be labeled as NNP -> norn, big as JJ -> adjective and run as VB -> verb
    tagged_list = []
    stop_words = set(stopwords.words("english"))
    for each_sent in tokenized_sent[5:]:
        filtered_words = list(
            filter(lambda w: w not in stop_words, word_tokenize(each_sent)))
        tagged = pos_tag(filtered_words)
        #print(tagged)
        tagged_list.append(tagged)

    return tagged_list

def chunking(tagged_data):
    #process the meaning of the sentence
    #usurally chunk to noun by regex -> grouping
    #<XX> -> XX denotes labeled tags generated by tag methods (tagging stage)
    chunkGram = r'''
        Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}
    ''' 

    chunk_parser = RegexpParser(chunkGram)
    chunked = chunk_parser.parse(tagged_data)
    print(chunked)
    #chunked.draw()

def chinking(tagged_data):
    # remove chunks from chunk you do not need (chink is also regular expression)
    # chink works with chunk together
    #the first line is chunk {} and the second line is chink }{
    chunkGram = r'''
        Chunk: {<.*>+}
                }<VB.*?|IN|DT|>+{
    ''' 
    chunk_parser = RegexpParser(chunkGram)
    chunked = chunk_parser.parse(tagged_data)
    print(chunked)

def name_endentity_chunk(tagged_data):
    # name identity recognization
    ne_en = ne_chunk(tagged_data, binary=True) #when binary is True -> no specific category like PERSON, ORGANIZATION, LOCATION... will show
    print(ne_en)
    #ne_en.draw()

def lemmatizer():
    # use more often than steming
    # without pos parameter, lemmatizer will check and change nouns's plural form to single form
    sample = ["Eric", "cats", "buildings", "ran", "running", "geese", "rocks", "feet", "better", "best"]
    lemmatizer = WordNetLemmatizer()
    ps = PorterStemmer()
    for each in sample:
        print(lemmatizer.lemmatize(each))
        print(ps.stem(lemmatizer.lemmatize(each)))

    # with pos as 'a' -> adj -> it will change the compare form to original form as example below: better to good
    # with pos as "v" -> verb
    # default pos is 'n' -> noun
    print(lemmatizer.lemmatize("better", pos='a'))
    print(lemmatizer.lemmatize("larger", pos='a'))
    print(lemmatizer.lemmatize("ran", pos='v'))
    print(lemmatizer.lemmatize("thought", pos='v'))

def corpus_test():
    #nltk.corpus has many sample text you can import -> from nltk.corpus import module name
    sample_text = gutenberg.raw("bible-kjv.txt")
    sent_tokens = sent_tokenize(sample_text)
    print(sent_tokens[5:15])

def wordnet_test():
    #synset -> synonyms of the word as the arg
    syns = wordnet.synsets("program")
    print(syns)
    print(syns[1].lemmas())
    print(syns[1].lemmas()[0].name())
    print(syns[1].definition())
    print(syns[0].examples())

    synss = []
    anyss = []

    for syn in wordnet.synsets("good"):
        for l in syn.lemmas():
            synss.append(l.name())
            if l.antonyms():
                anyss.append(l.antonyms()[0].name())

    print(set(synss))
    print(set(anyss))

    w1 = wordnet.synset("ship.n.01")
    w2 = wordnet.synset("boat.n.01")
    #measure meaning similarity
    print("the meaning similarity between {} and {} is {:.2f}".format(w1, w2, w1.wup_similarity(w2)))

    w3 = wordnet.synset("car.n.01")
    #measure meaning similarity
    print("the meaning similarity between {} and {} is {:.2f}".format(w1, w3, w1.wup_similarity(w3)))

    w4 = wordnet.synset("human.n.01")
    #measure meaning similarity
    print("the meaning similarity between {} and {} is {:.2f}".format(w1, w4, w1.wup_similarity(w4)))

def find_features(doc, word_features):
    words = set(doc)
    features = dict()
    for w in word_features:
        features[w] = (w in words)

    return features

def text_classify():
    #test against documents
    documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]
    #print(documents[0])
    random.shuffle(documents)

    #find most popular words in pos and neg categories
    all_words = [w.lower() for w in movie_reviews.words()]
    #print(all_words)

    stop_words = set(stopwords.words("english"))
    all_words = list(filter(lambda w: w not in stop_words, all_words))
    
    #statis of words 
    all_words = FreqDist(all_words)
    #print(all_words.most_common(10))
    #print(all_words['stupid'])

    #print(len(all_words))

    word_features = list(all_words.keys())[:4000]

    features_sets = list((find_features(rev, word_features), cat) for rev, cat in documents)

    # features_sets: labeled data from documents
    cut = int(0.8 * len(features_sets))
    training_set = features_sets[:cut]
    testing_set = features_sets[cut:]

    #posterior  = prior occurences X likelihood / evidence
    #train
    classifier = NaiveBayesClassifier.train(training_set)
    print("accuracy: ", accuracy(classifier, testing_set))
    #classifier.show_most_informative_features(3)

    # save trained model as piclke obj
    with open("NaiveBayes.pkl", "wb") as f:
        pickle.dump(classifier, f) 
    
    #load training model
    with open("NaiveBayes.pkl", "rb") as f:
        cf = pickle.load(f)
    print("new accuracy: ", accuracy(cf, testing_set))
    #cf.show_most_informative_features(3)

    #using classifying method from sklearn
    MNB_cf = SklearnClassifier(MultinomialNB())
    MNB_cf.train(training_set)
    print("MultinomialNB accuracy: ", accuracy(MNB_cf, testing_set))

    # GNB_cf = SklearnClassifier(GaussianNB())
    # GNB_cf.train(training_set)
    # print("GaussianNB accuracy: ", accuracy(GNB_cf, testing_set))

    BNB_cf = SklearnClassifier(BernoulliNB())
    BNB_cf.train(training_set)
    print("BernoulliNB accuracy: ", accuracy(BNB_cf, testing_set))

# LogisticRegression, SGDClassifier
# SVC, LinearSVC, NuSVC
    LR_cf = SklearnClassifier(LogisticRegression())
    LR_cf.train(training_set)
    print("LogisticRegression accuracy: ", accuracy(LR_cf, testing_set))

    SGDClassifier_cf = SklearnClassifier(SGDClassifier())
    SGDClassifier_cf.train(training_set)
    print("SGDClassifier accuracy: ", accuracy(SGDClassifier_cf, testing_set))

    SVC_cf = SklearnClassifier(SVC())
    SVC_cf.train(training_set)
    print("SVC accuracy: ", accuracy(SVC_cf, testing_set))

    LinearSVC_cf = SklearnClassifier(LinearSVC())
    LinearSVC_cf.train(training_set)
    print("LinearSVC accuracy: ", accuracy(LinearSVC_cf, testing_set))

    NuSVC_cf = SklearnClassifier(NuSVC())
    NuSVC_cf.train(training_set)
    print("NuSVC accuracy: ", accuracy(NuSVC_cf, testing_set))

    vote_cf = VoteClassifier(classifier, MNB_cf, BNB_cf, LR_cf, SGDClassifier_cf, SVC_cf, LinearSVC_cf, LinearSVC_cf, NuSVC_cf)
    print("voted classifier accuracy: ", accuracy(vote_cf, testing_set))

    #some example one single records
    print("Classification: ", vote_cf.classify(testing_set[1][0]), "confidence: ", vote_cf.confidence(testing_set[1][0]))
    print("Classification: ", vote_cf.classify(testing_set[2][0]), "confidence: ", vote_cf.confidence(testing_set[2][0]))
    print("Classification: ", vote_cf.classify(testing_set[3][0]), "confidence: ", vote_cf.confidence(testing_set[3][0]))
    print("Classification: ", vote_cf.classify(testing_set[5][0]), "confidence: ", vote_cf.confidence(testing_set[5][0]))
    print("Classification: ", vote_cf.classify(testing_set[8][0]), "confidence: ", vote_cf.confidence(testing_set[8][0]))

    # for i in range(len(testing_set)):
    #     if vote_cf.confidence(testing_set[i][0]) != 1:
    #         print("Classification: ", vote_cf.classify(testing_set[i][0]), "confidence: ", vote_cf.confidence(testing_set[i][0]))

# create vote classifier class
class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self.classifiers = classifiers

    def classify(self, features):
        votes = []
        for classifier in self.classifiers:
            v = classifier.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for classifier in self.classifiers:
            v = classifier.classify(features)
            votes.append(v)
        choice_vote = votes.count(mode(votes))
        return choice_vote / len(votes)

def main():
    # tokenize()
    
    # stop_words_filter()
    
    # steming()
    
    # tagged_data = speech_tagging()
    # for i, each in enumerate(tagged_data):
    #     if i == 1:
    #         chunking(each)
    #         chinking(each)
    #         name_endentity_chunk(each)

    #lemmatizer()
    
    #corpus_test()
    
    #wordnet_test()

    text_classify()

if __name__ == '__main__':
    main()
