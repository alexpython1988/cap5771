{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project\n",
    "### Xi Yang 16216573\n",
    "\n",
    "### data\n",
    "** the data set are from i2b2 which needs auth to public, so I could not shared the data, but example data are described below **\n",
    "- training data: C:\\Users\\xiyang\\Google Drive\\NLP_Project\\i2b2_data\\concept_assertion_relation_training_data\n",
    "    1. concept: beth/concept/*.con; partners/concept/*.con\n",
    "    2. txt: beth/txt/*.txt; partners/txt/*.txt\n",
    "- testing data: C:\\Users\\xiyang\\Google Drive\\NLP_Project\\i2b2_data\\reference_standard_for_test_data\n",
    "    1. concept: concepts/*.con\n",
    "    2. txt: test_data/*.txt\n",
    "\n",
    "### data sample\n",
    "** .con and .txt files are linked by sharing same filename **\n",
    "- data in .con file: c=\"angap\" 65:25 65:25||t=\"test\"\n",
    "- data in .txt file: 2018-10-31 06:25 AM BLOOD Glucose - 91 UreaN - 19 Creat - 0.8 Na - 134 K - 4.0 Cl - 98 HCO3 - 26 _AnGap_ - 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "#read in training and testing data as raw\n",
    "import glob\n",
    "from functools import reduce\n",
    "\n",
    "train_base_path = \"C:\\\\Users\\\\xiyang\\\\Google Drive\\\\NLP_Project\\\\i2b2_data\\\\concept_assertion_relation_training_data\"\n",
    "\n",
    "#print(list(map(lambda x: glob.glob(x),  [files for files in con_file_paths])))\n",
    "\n",
    "train_con_file_paths = [train_base_path + \"\\\\beth\\\\concept\\\\*.con\", train_base_path + \"\\\\partners\\\\concept\\\\*.con\"]\n",
    "train_txt_file_paths = [train_base_path + \"\\\\beth\\\\txt\\\\*.txt\", train_base_path + \"\\\\partners\\\\txt\\\\*.txt\"]\n",
    "\n",
    "train_con_files = list(reduce(lambda y, z: y + z, list(map(lambda x: glob.glob(x),  [files for files in train_con_file_paths]))))\n",
    "train_txt_files = list(reduce(lambda y, z: y + z, list(map(lambda x: glob.glob(x),  [files for files in train_txt_file_paths]))))\n",
    "\n",
    "#print(len(con_files))\n",
    "#print(len(txt_files))\n",
    "\n",
    "#in each file pair, the first is the txt and second is the associated concept file\n",
    "train_file_pairs = list(zip(train_txt_files, train_con_files))\n",
    "print(len(train_file_pairs))\n",
    "\n",
    "\n",
    "test_base_path = \"C:\\\\Users\\\\xiyang\\\\Google Drive\\\\NLP_Project\\\\i2b2_data\\\\reference_standard_for_test_data\"\n",
    "\n",
    "#print(list(map(lambda x: glob.glob(x),  [files for files in con_file_paths])))\n",
    "\n",
    "test_con_file_paths = [test_base_path + \"\\\\concepts\\\\*.con\"]\n",
    "test_txt_file_paths = [test_base_path + \"\\\\test_data\\\\*.txt\"]\n",
    "\n",
    "test_con_files = list(reduce(lambda y, z: y + z, list(map(lambda x: glob.glob(x),  [files for files in test_con_file_paths]))))\n",
    "test_txt_files = list(reduce(lambda y, z: y + z, list(map(lambda x: glob.glob(x),  [files for files in test_txt_file_paths]))))\n",
    "\n",
    "#print(len(con_files))\n",
    "#print(len(txt_files))\n",
    "\n",
    "#in each file pair, the first is the txt and second is the associated concept file\n",
    "test_file_pairs = list(zip(test_txt_files,test_con_files))\n",
    "print(len(test_file_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In txt file, each line is a sentence token and each word need to be tokenized\n",
    "tokenize line into words listed in each line and pedding related tags based on information from con file\n",
    "using BIO tagging\n",
    "Example:\n",
    "    concept: c=\"angap\" 65:25 65:25||t=\"test\"\n",
    "    sentence: 98 HCO3 - 26 AnGap - 14\n",
    "    output:\n",
    "    98    o\n",
    "    HCO3    o\n",
    "    -    o\n",
    "    26    o\n",
    "    ANGap    B-test\n",
    "    -    o\n",
    "    14   o\n",
    "'''\n",
    "import re\n",
    "import collections\n",
    "\n",
    "def preprocess_raw_con_txt_file(file_pair):\n",
    "    con_dict = dict()\n",
    "    con_tag_dict = dict()\n",
    "    txt = file_pair[0]\n",
    "    con = file_pair[1]\n",
    "    \n",
    "    with open(con, \"r\") as fr:\n",
    "        for i, line in enumerate(fr):\n",
    "            info1 = line.split(\"||\")\n",
    "            #info2 = info1[0].split(\" \")\n",
    "            #print(re.findall(r'\\\"(.+?)\\\"', info1[0])[0])\n",
    "            info2 = re.findall(r'[0-9]+:[0-9]+', info1[0])\n",
    "            #print(info2)\n",
    "            #get concept\n",
    "            c = re.findall(r'\\\"(.+?)\\\"', info1[0])[0]\n",
    "            \n",
    "            #get tag label\n",
    "            tl = re.findall(r'\\\"(.+?)\\\"', info1[1])[0]\n",
    "            \n",
    "            #tag B and I\n",
    "            first_token_num = int(info2[0].split(\":\")[1])\n",
    "            last_token_num = int(info2[1].split(\":\")[1])\n",
    "            span = last_token_num - first_token_num\n",
    "            \n",
    "            line_num = int(info2[0].split(\":\")[0])\n",
    "            \n",
    "            if line_num not in con_dict:\n",
    "                B_tags = dict()\n",
    "                I_tags = dict()\n",
    "                B_tags[first_token_num] = tl\n",
    "                for i in range(span):\n",
    "                    I_tags[first_token_num+i+1] = tl\n",
    "                con_dict[line_num] = dict()\n",
    "                con_dict[line_num]['B_tags'] = B_tags\n",
    "                con_dict[line_num]['I_tags'] = I_tags\n",
    "            else:\n",
    "                con_dict[line_num]['B_tags'][first_token_num] = tl\n",
    "                for i in range(span):\n",
    "                     con_dict[line_num]['I_tags'][first_token_num+i+1] = tl\n",
    "    #print(con_dict)\n",
    "            \n",
    "    with open(txt, \"r\") as fr:\n",
    "        for line_index, line in enumerate(fr):\n",
    "            tokens = line[:-1].split(\" \")\n",
    "            con_tag_dict[line_index+1] = []\n",
    "            if line_index+1 in con_dict:\n",
    "                ref_B_tags = con_dict[line_index+1]['B_tags']\n",
    "                ref_I_tags = con_dict[line_index+1]['I_tags']\n",
    "                #print(ref_B_tags)\n",
    "                for token_index, token in enumerate(tokens):\n",
    "                    if token_index in ref_B_tags:\n",
    "                        new_tag = \"-\".join([\"B\", ref_B_tags[token_index]])\n",
    "                    elif token_index in ref_I_tags:\n",
    "                        new_tag = \"-\".join([\"I\", ref_I_tags[token_index]])\n",
    "                    else:\n",
    "                        new_tag = \"O\"\n",
    "                    new_token = \" \".join([token, new_tag])\n",
    "                    #print(new_token)\n",
    "                    con_tag_dict[line_index+1].append(new_token)\n",
    "            else:\n",
    "                for token in tokens:\n",
    "                    new_token = \" \".join([token, \"O\"])\n",
    "                    con_tag_dict[line_index+1].append(new_token)\n",
    "    \n",
    "    return collections.OrderedDict(sorted(con_tag_dict.items()))\n",
    "    \n",
    "#test function\n",
    "# p = ('C:\\\\Users\\\\xiyang\\\\Google Drive\\\\NLP_Project\\\\i2b2_data\\\\concept_assertion_relation_training_data\\\\beth\\\\txt\\\\record-105.txt', 'C:\\\\Users\\\\xiyang\\\\Google Drive\\\\NLP_Project\\\\i2b2_data\\\\concept_assertion_relation_training_data\\\\beth\\\\concept\\\\record-105.con')\n",
    "# print(preprocess_raw_con_txt_file(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "create file with cornell-2000 format for training and testing\n",
    "'''\n",
    "train_file_name = \"train.txt\"\n",
    "test_file_name = \"test.txt\"\n",
    "\n",
    "def create_input(file_pairs, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for pair in file_pairs:\n",
    "            tags = preprocess_raw_con_txt_file(pair)\n",
    "            for k, v in tags.items():\n",
    "                #print(k)\n",
    "                for each in v:\n",
    "                    print(each, file=f, end='\\n')\n",
    "                print('', file=f, end='\\n')\n",
    "            \n",
    "create_input(train_file_pairs, train_file_name)\n",
    "create_input(test_file_pairs, test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7c9a2c5c6288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_file_pairs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#get train data word freq and word cloud\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "for each in train_file_pairs:\n",
    "    with open(each[0], \"r\") as fr:\n",
    "        for line in fr:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
